{
  "hash": "2720645bd1fc0c184a1dccea0eb0f791",
  "result": {
    "markdown": "---\ntitle: 'Building Blocks of Transformers: 2. Position Representation'\nformat:\n  html:\n    code-fold: true\nexecute:\n  echo: false\nauthor: Haad Khan\ndate: '2023-11-11'\ncategories:\n  - Attention\n  - Transformers\n  - Machine Learning\n---\n\nGiven the sequence \"He can always be seen working hard.\" This sequence is distinctly different from \"He can hardly be seen working\". \nSubtle change in the position of the word can make a distinct difference in the meaning. \n\nRNN's are traditionally deployed for NLP because the order of the sequence defines the order of the rollout. Thus it is capable of \nrepresenting two different sequences. \n\nSelf attention is devoid of the notion of the order of the sequence. \n\nPosition representation through learned embeddings. \nA common way to encode the position of the word/token is to use vectors that are already position dependent\n P ∈ RN×d,\n\n We then simply add embedded representation of the position of a\nword to its word embedding:\n˜xi = Pi + xi (12)\n\nand perform self-attention as we otherwise would. Now, the self-\nattention operation can use the embedding Pi to look at the word at\nposition i differently than if that word were at position j. This is done,\ne.g., in the BERT paper [ Devlin et al., 2019] \n\nAnother way to do this is to change the self attention operation itself. \n\n2.3 Elementwise nonlinearity\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
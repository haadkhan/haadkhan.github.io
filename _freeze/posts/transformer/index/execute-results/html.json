{
  "hash": "fbd04a890a287d92fe4c06aa4f914eac",
  "result": {
    "markdown": "---\ntitle: 'Building Blocks of Transformers: 1. Self Attention'\nformat:\n  html:\n    code-fold: true\nauthor: Haad Khan\ndate: '2023-10-01'\ncategories:\n  - Attention\n  - Transformers\n  - Machine Learning\n---\n\n<p>I had a garbled understanding of Transformer architecture after consulting blogs, videos and coursera course. What made it finally clicked for me is Stanford CS 224N lecture by John Hewitt. He does a phenomenal job and I will strongly encourage to check it out. If you work with LLM's the 1 hour and 17 minutes is worth the time investment.</p> \n[Stanford CS224N NLP | Lecture 8 - Self-Attention and Transformers](https://youtu.be/nTQUwghvy5Q)\n<p>Before diving into the details of transformer a little history.<br>\n\n### Motivation for Attention\n\nPrior to introduction of Transformers, the state of the art algorithm for acheiving state of the art results on various NLP tasks were RNN's and its variants e.g LSTM, BiDirectional LSTM etc.\n\nWhile the sequential nature of RNN lend itself well to modelling sequential data, it had some issues.\nLinear interaction distance\nLack of parallelizability\n\n\n</p>\n\n::: {#attention-implementation .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"Scaled Dot-Product Attention\"\"\"\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        output = torch.matmul(attn, v)\n\n        return output, attn\n\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
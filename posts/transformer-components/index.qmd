---
title: "Building Blocks of Transformers: 2. Position Representation"
format:
  html:
    code-fold: true
execute:
  echo: false
jupyter: python3
author: "Haad Khan"
date: "2023-11-12"
categories: [Attention, Transformers, Machine Learning]
---
Given the sequence "He can always be seen working hard." This sequence is distinctly different from "He can hardly be seen working". 
Subtle change in the position of the word can make a distinct difference in the meaning. 

RNN's are traditionally deployed for NLP because the order of the sequence defines the order of the rollout. Thus it is capable of 
representing two different sequences. 

Self attention is devoid of the notion of the order of the sequence. 

Position representation through learned embeddings. 
A common way to encode the position of the word/token is to use vectors that are already position dependent
 P ∈ RN×d,

 We then simply add embedded representation of the position of a
word to its word embedding:
˜xi = Pi + xi (12)

and perform self-attention as we otherwise would. Now, the self-
attention operation can use the embedding Pi to look at the word at
position i differently than if that word were at position j. This is done,
e.g., in the BERT paper [ Devlin et al., 2019] 

Another way to do this is to change the self attention operation itself. 

2.3 Elementwise nonlinearity


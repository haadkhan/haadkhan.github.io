---
title: "Building Blocks of Transformers: 1. Self Attention"
format:
  html:
    code-fold: true
execute:
  echo: false
jupyter: python3
author: "Haad Khan"
date: "2023-10-01"
categories: [Attention, Transformers, Machine Learning]
---

<p>I had a garbled understanding of Transformer architecture after consulting blogs, videos and coursera course. What made it finally clicked for me is Stanford CS 224N lecture by John Hewitt. He does a phenomenal job and I will strongly encourage to check it out. If you work with LLM's the 1 hour and 17 minutes is worth the time investment.</p> 
[Stanford CS224N NLP | Lecture 8 - Self-Attention and Transformers](https://youtu.be/nTQUwghvy5Q)
<p>Before diving into the details of transformer a little history.<br></p>

#### Motivation for Transformers

<p>Prior to introduction of Transformers, the state of the art algorithm for acheiving state of the art results on various NLP tasks were RNN's and its variants e.g LSTM, BiDirectional LSTM etc.
While the sequential nature of RNN lend itself well to modelling sequential data, it had some issues. The major ones being the following.</br>

**a). Linear interaction distance**

**b). Lack of parallelizability**

Transformer solve the above problems but have their own issues which will come up later. Transformer is illustrated by the diagram below.</br>
 ![transformer-model-architecture](The-Transformer-model-architecture.jpg){width=300}

The diagram is complex but when approached as a submodules the algorithm is composed of its starts to make sense. In this note the submodule we will look at is Self-Attention.

The words from natural language go through a text tokenizer. Numerous technqiues exist for tokenize, but a very common method for tokenizing text is byte pair encoding. [Here](https://huggingface.co/learn/nlp-course/chapter6/5) is a good explainer for byte pair encoding. 

The tokenizer will transform a sentence into a list of numeral integers or a list of tokens. Given a token $x_i$ in the list of tokens $x_1:n$ we define a query $q_i = Qx_i$, for matrix Q. A vector key and value are also needed such that $k_i = Kx_i$ and $v_i = Vx_i$.

The eventual representation of the list of tokens  $h_i$ is dot product of the value of the sequence. 
$$
h_i = \sum_{j=1}^{n} \alpha_{ij} v_j,
$$
The weights $\alpha_{ij}$ selects the data to pull up. The weights are defined by calculating the relation between key and query $q_i^Tk_j$ and calculating the softmax over the sequence.
$$
\alpha_ij = \frac {exp(q_i^T k_j)}{\sum_{j^`=1}^{n}exp(q_i^Tk_{j^`})}
$$

Self-attention instead of using a fixed embedding for each token, leverages the whole sequence to determine what other tokens should represent $x_i$ in context. 

**References:**
<p>1). https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#/media/File:The-Transformer-model-architecture.png</p>
<p>2). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017; <a href='http://arxiv.org/abs/1706.03762'>arXiv:1706.03762</a></p>
<p>3). [Stanford CS224N NLP](http://web.stanford.edu/class/cs224n/index.html)</p>
<p>4). [Stanford CS224N NLP | Self-Attention and Transformers](http://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)</p>
<p>5). https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt</p>
---
title: "Counting Number of Parameters in GPT2"
format:
  html:
    code-fold: false
execute:
  enabled: false
jupyter: python3
author: "Haad Khan"
date: "2024-05-29"
categories: [Attention, Transformers, GPT2, Math]
---
<p>While going through the [Let's reproduce GPT-2(124M)](https://youtu.be/l8pRSuU81PU) I fell into the rabit hole of how the 124M parameter are distributed across the decoder of the transformer. 

```{python eval=false}
!pip install transformers torch
```

```{python eval=false}
from transformers import GPT2Model
model = GPT2Model.from_pretrained('openai-community/gpt2')
```

<p>Model String object contains the following information about the architecture.</p>
```{python eval=false}
print(model)
```
```
GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0-11): 12 x GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2SdpaAttention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
```
<p>Lets write a utility to sum all the parameters of the model. I am using the numerize library to print in a more eye friendly format.</p>

```{python eval=false}
from numerize import numerize as n
total_params = sum(p.numel() for p in model.parameters())
print("number of params in model: ", n.numerize(total_params))
```
```
number of params in model: 124.44M
```
Knowing the total number of parameters, lets break it down on a per layer basis. Before we do that, lets lay out some of the constants of the overall architecture. 

<p>Vocabulary size $(V)$: 50,257</p>
<p>Embedding dimension $(D)$: 768</p>
<p>Number of attention heads $(H)$: 12</p>
<p>Context Length $(C)$: 1024</p>
<p>Number of layers $(L)$: 12</p>


## Parameter Breakdown Per layer
The first two layers are embedding layer wte and wpe. 

### Embedding Layer (wte)
The model starts with an embedding layer that converts input tokens (words) into continuous representations.

If the vocabulary size is V and the embedding dimension is D, this layer has V×D parameters.

No. of param = V x D = 50257 * 768 = 38 597 376

### Embedding Layer (wpe)
Recall the notion of Positional Embedding. Since transformers by itself don't encode the position, this layer embeds the position of input tokens. 

If the context size is C and the embedding dimension is D, this layer has C×D parameters.

No. of param = C x D = 1024 * 768 = 786 432

To verify calculation, the function used to calculate for the entire model can be modified as following. 

```{python eval=false}
total_params = sum(p.numel() for p in model._modules['wte'].parameters())
print("number of params in model embedding wte: ", n.numerize(total_params))
total_params = sum(p.numel() for p in model._modules['wpe'].parameters())
print("number of params in model embedding wpe: ", n.numerize(total_params))
```
```
number of params in model embedding wte:  38.6M
number of params in model embedding wpe:  786.43K
```

## Transformer layers
We have acheived the conversion of text tokens into an embedding. Next step is to apply the self attention operation. GPT2Model defines 12 attention blocks starting with the layer norm. 

### Layer Norm (ln_1)
Each transformer block starts with a layer normalization steps that have parameters for scaling and shifting the normalized output, each adding \(2D\) parameters per block.

No. of param = 2xD = 2 x 768 = 1536

### Attention c_attn
      - The attention mechanism has \( H \) heads, each with its own set of weights for query, key, and value transformations.
       - For each head, if the hidden size is \( D \), then each of the query, key, and value transformations has \( D \times D \) parameters.
       - Additionally, there is a projection layer that combines the outputs of the heads, adding another \( D \times D \) parameters.

No. of param = [Dx(3xD)+(3xD)] = 1771776

### Attention c_proj
It is responsible for combining the outputs of the attention heads (in our case, there are 12 heads amongst which 768 dims are equally divided, which gives each head a 64-dim output).

No. of param = [DxDxD] = 590592
params = c_attn + c_proj = [Dx(3xD)+(3xD)] + [DxDxD] = 2362368

```{python eval=false}
c_attn_params = sum(p.numel() for p in model._modules['h'][0].attn.c_attn.parameters())
c_proj_params = sum(p.numel() for p in model._modules['h'][0].attn.c_proj.parameters())
print("number of params in c_attn: ", n.numerize(c_attn_params ))
print("number of params in c_proj: ", n.numerize(c_proj_params ))
print("sum of c_attn and c_proj", n.numerize(c_attn_params+c_proj_params))
```
```
number of params in c_attn:  1.77M
number of params in c_proj:  590.59K
sum of c_attn and c_proj 2.36M
```

### Layer Norm (ln_2)
Its similar to Layer Norm ln_1

### Feed Forward Neural Network mlp
c_fc = [Dx(4xD)+(4xD)] = 2362368
c_proj = [(4xD)xD+D] = 2360064
No. of param = [Dx(4xD)+(4xD)] + [(4xD)xD+D] = 4722432
```{python eval=false}
c_fc_params = sum(p.numel() for p in model._modules['h'][0].mlp.c_fc.parameters())
c_proj_params = sum(p.numel() for p in model._modules['h'][0].mlp.c_proj.parameters())
print("number of params in c_attn: ", n.numerize(c_fc_params ))
print("number of params in c_proj: ", n.numerize(c_proj_params ))
print("number of params in mlp", n.numerize(c_fc_params +c_proj_params))
```
```
number of params in c_attn:  2.36M
number of params in c_proj:  2.36M
number of params in mlp 4.72M
```

### Layer Norm (ln_f)
Its similar to Layer Norm ln_1

## Total number of parameters in GPT2
To calculate params in GPT2 we have to add all the parameters we have come across.


No. of param = wte + cte + Lx(ln_1 + attn + ln_2 + mlp) + ln_f
             = 38597376 + 786432 + 12x(1536 + 2362368 + 1536 + 4722432) + 1536
             = 124 439 808

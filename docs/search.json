[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Haad, an applied scientist working at the intersection of LLM and Healthcare. Outside of work he can be found on a sand court playing üèê or pickleball üèì\nThe name of the blog ‚Äúbyte sized ML‚Äù is inspired from byte sized engineering https://www.bytesizedengineering.com/ ## Education\nUniversity of Michigan | Ann Arbor MI MS in NLP & Information Retreival | 2014 - 2016\nGIK University B.Eng | Aug 2010 - May 2014"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nOracle | Sr.¬†Applied Scientist | Mar 2018 - present Cerebri AI | Data Scientist | Feb 2016 - Feb 2018"
  },
  {
    "objectID": "posts/transformer/index.html",
    "href": "posts/transformer/index.html",
    "title": "Building Blocks of Transformers: 1. Self Attention",
    "section": "",
    "text": "I had a garbled understanding of Transformer architecture after consulting blogs, videos and coursera course. What made it finally clicked for me is Stanford CS 224N lecture by John Hewitt. He does a phenomenal job and I will strongly encourage to check it out. If you work with LLM‚Äôs the 1 hour and 17 minutes is worth the time investment.\n\nStanford CS224N NLP | Lecture 8 - Self-Attention and Transformers\n\nBefore diving into the details of transformer a little history.\n\nMotivation for Attention\nPrior to introduction of Transformers, the state of the art algorithm for acheiving state of the art results on various NLP tasks were RNN‚Äôs and its variants e.g LSTM, BiDirectional LSTM etc.\nWhile the sequential nature of RNN lend itself well to modelling sequential data, it had some issues. Linear interaction distance Lack of parallelizability\n\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"Scaled Dot-Product Attention\"\"\"\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        output = torch.matmul(attn, v)\n\n        return output, attn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Byte Sized ML",
    "section": "",
    "text": "Building Blocks of Transformers: 1. Self Attention\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nHaad Khan\n\n\n\n\n\n\nNo matching items"
  }
]
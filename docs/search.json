[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Haad, an applied scientist working at the intersection of LLM and Healthcare. Outside of work he can be found on a sand court playing üèê or pickleball üèì\nThe name of the blog ‚Äúbyte sized ML‚Äù is inspired from byte sized engineering https://www.bytesizedengineering.com/ ## Education\nUniversity of Michigan | Ann Arbor MI MS in NLP & Information Retreival | 2014 - 2016\nGIK University B.Eng | Aug 2010 - May 2014"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nOracle | Sr.¬†Applied Scientist | Mar 2018 - Nov 2023 Cerebri AI | Data Scientist | Feb 2016 - Feb 2018"
  },
  {
    "objectID": "posts/transformer-components/index.html",
    "href": "posts/transformer-components/index.html",
    "title": "Building Blocks of Transformers: 2. Position Representation",
    "section": "",
    "text": "1. Positional Encoding Given the sequence He can always be seen working hard. This sequence is distinctly different from He can hardly be seen working. As can be seen slight change in the position of the word conveys a drastically different meaning.\nRNN‚Äôs are traditionally deployed for NLP because the order of the sequence defines the order of the rollout. Thus it is capable of representing two different sequences. Vanilla Self-attention lacks the notion of the order of the sequence. This can be seen from the attention operation on both the sequences. The first sequence \\(s_0\\) can be represented as set of vectors \\[\n\\textbf{x}_{1:n} = [ \\textbf{x}_{He};\\textbf{x}_{can};\\textbf{x}_{always};\\textbf{x}_{be} ;\\textbf{x}_{seen};\\textbf{x}_{working};\\textbf{x}_{hard};\\textbf{x}_{s1}] \\in \\mathrm{R^{7xd}}\n\\] The softmax or the weight \\(\\alpha_{s_{1,0}}\\) by which we look up first word by \\[\n\\alpha_{s1} = softmax([q^\\intercal_{s1}k_{He};q^\\intercal_{s1}k_{can};q^\\intercal_{s1}k_{always};\nq^\\intercal_{s1}k_{be} ;\nq^\\intercal_{s1}k_{seen};\nq^\\intercal_{s1}k_{working};\nq^\\intercal_{s1}k_{hard};\nq^\\intercal_{s1}k_{s1}])\n\\] For the reordered sentence \\(s_0\\) He can hardly be seen working. The \\(\\alpha_{s2}\\) will be identical to attention of the first sequence since the numerator hasn‚Äôt changed, and the denominator hasn‚Äôt changed; only the terms are rearranged in the sum. This all comes back down to the two facts that (1) the representation of x is not position- dependent; it‚Äôs just Ew for whatever word w, and (2) there‚Äôs no dependence on position in the self-attention operations.\nRepresentating Position via learned embedding&lt;&gt; A common way to encode the position of the word/token is to encode positional information in the input vector itself. \\[\n\\tilde{x_{i}} = P_{i} + x_{i}\n\\] And perform self attention on the position encoded vector.\n2 Nonlinear element-wise operation Consider the scenario where multiple layers of self-attention are piled on top of each other. Could this arrangement potentially serve as a substitute for the traditionally used stacked LSTM layers? There appears to be a missing component when we think about this intuitively - the elementwise nonlinear functions that are a staple in typical deep learning models. Interestingly, when you combine two layers of self-attention, the resulting structure bears a striking resemblance to that of a solitary self-attention layer.\nIt‚Äôs usual to follow a self-attention layer with a feed-forward network that processes each word representation individually. \\[\nh_{FF} = W_2 ReLU(W1h_{self-attention} + b1) + b2\n\\] 3 Future masking\nIn a Transformer, there‚Äôs nothing explicit in the self-attention weight Œ± that says not to look at indices j &gt; i when representing token i. In practice, we enforce this constraint simply adding a large negative constant to the input to the softmax.\n\\[\na_{ij,masked} = \\left\\{ \\begin{array}{cl}\na_{ij} & : \\ j \\leq i \\\\\n0 & : \\ otherwise\n\\end{array} \\right.\n\\]"
  },
  {
    "objectID": "posts/transformer/index.html",
    "href": "posts/transformer/index.html",
    "title": "Building Blocks of Transformers: 1. Self Attention",
    "section": "",
    "text": "I had a garbled understanding of Transformer architecture after consulting blogs, videos and coursera course. What made it finally clicked for me is Stanford CS 224N lecture by John Hewitt. He does a phenomenal job and I will strongly encourage to check it out. If you work with LLM‚Äôs the 1 hour and 17 minutes is worth the time investment.\n\nStanford CS224N NLP | Lecture 8 - Self-Attention and Transformers\n\nBefore diving into the details of transformer a little history.\n\n\nMotivation for Transformers\n\nPrior to introduction of Transformers, the state of the art algorithm for acheiving state of the art results on various NLP tasks were RNN‚Äôs and its variants e.g LSTM, BiDirectional LSTM etc. While the sequential nature of RNN lend itself well to modelling sequential data, it had some issues. The major ones being the following.\na). Linear interaction distance\nb). Lack of parallelizability\nTransformer solve the above problems but have their own issues which will come up later. Transformer is illustrated by the diagram below. \nThe diagram is complex but when approached as a submodules the algorithm is composed of its starts to make sense. In this note the submodule we will look at is Self-Attention.\nThe words from natural language go through a text tokenizer. Numerous technqiues exist for tokenize, but a very common method for tokenizing text is byte pair encoding. Here is a good explainer for byte pair encoding.\nThe tokenizer will transform a sentence into a list of numeral integers or a list of tokens. Given a token \\(x_i\\) in the list of tokens \\(x_1:n\\) we define a query \\(q_i = Qx_i\\), for matrix Q. A vector key and value are also needed such that \\(k_i = Kx_i\\) and \\(v_i = Vx_i\\).\nThe eventual representation of the list of tokens \\(h_i\\) is dot product of the value of the sequence. \\[\nh_i = \\sum_{j=1}^{n} \\alpha_{ij} v_j,\n\\] The weights \\(\\alpha_{ij}\\) selects the data to pull up. The weights are defined by calculating the relation between key and query \\(q_i^Tk_j\\) and calculating the softmax over the sequence. \\[\n\\alpha_ij = \\frac {exp(q_i^T k_j)}{\\sum_{j^`=1}^{n}exp(q_i^Tk_{j^`})}\n\\]\nSelf-attention instead of using a fixed embedding for each token, leverages the whole sequence to determine what other tokens should represent \\(x_i\\) in context.\nReferences:\n\n1). https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#/media/File:The-Transformer-model-architecture.png\n\n\n2). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: ‚ÄúAttention Is All You Need‚Äù, 2017; arXiv:1706.03762\n\n\n3). Stanford CS224N NLP\n\n\n4). Stanford CS224N NLP | Self-Attention and Transformers\n\n\n5). https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
  },
  {
    "objectID": "posts/transformer-params/index.html",
    "href": "posts/transformer-params/index.html",
    "title": "Counting Number of Parameters in GPT2",
    "section": "",
    "text": "While going through the Let‚Äôs reproduce GPT-2(124M) I fell into the rabit hole of how the 124M parameter are distributed across the decoder of the transformer.\n!pip install transformers torch\nfrom transformers import GPT2Model\nmodel = GPT2Model.from_pretrained('openai-community/gpt2')\nModel String object contains the following information about the architecture.\nprint(model)\nLets write a utility to sum all the parameters of the model. I am using the numerize library to print in a more eye friendly format.\nfrom numerize import numerize as n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(\"number of params in model: \", n.numerize(total_params))\nThis is the total number of parameters. To understand how these parameters are distributed lets layout all the pieces of transformer, find out the number of parameters for the individual piece. At the end, for sanity check, we can add the number of parameters across all components to ensure we get the number above.\nIt‚Äôs helpful to define constants so later, we can just plug in the values.\nVocabulary size \\((V)\\): 50,257\nEmbedding dimension \\((D)\\): 768\nNumber of attention heads \\((H)\\): 12\nContext Length \\((C)\\): 1,024\nNumber of layers \\((L)\\): 12"
  },
  {
    "objectID": "posts/transformer-params/index.html#parameter-breakdown-per-layer",
    "href": "posts/transformer-params/index.html#parameter-breakdown-per-layer",
    "title": "Counting Number of Parameters in GPT2",
    "section": "Parameter Breakdown Per layer",
    "text": "Parameter Breakdown Per layer\nThe first two layers are embedding layer wte and wpe.\n\nEmbedding Layer (wte)\n\nThe model starts with an embedding layer that converts input tokens (words) into continuous representations.\n\n\nIf the vocabulary size is V and the embedding dimension is D, this layer has V√óD parameters.\n\n\nNo.¬†of param = V x D = 50,257 x 768 = 38,597,376\n\n\n\nEmbedding Layer (wpe)\n\nRecall the notion of Positional Embedding. Since transformers by itself don‚Äôt encode the position, this layer embeds the position of input tokens.\n\n\nIf the context size is C and the embedding dimension is D, this layer has C√óD parameters.\n\n\nNo.¬†of param = C x D = 1,024x768 = 786,432\n\n\nTo verify calculation, the function used to calculate for the entire model can be modified as following.\n\n\ntotal_params = sum(p.numel() for p in model._modules['wte'].parameters())\nprint(\"number of params in model embedding wte: \", n.numerize(total_params))\ntotal_params = sum(p.numel() for p in model._modules['wpe'].parameters())\nprint(\"number of params in model embedding wpe: \", n.numerize(total_params))\n\nnumber of params in model embedding wte:  38.6M\nnumber of params in model embedding wpe:  786.43K"
  },
  {
    "objectID": "posts/transformer-params/index.html#transformer-layers",
    "href": "posts/transformer-params/index.html#transformer-layers",
    "title": "Counting Number of Parameters in GPT2",
    "section": "Transformer layers",
    "text": "Transformer layers\nWe have acheived the conversion of text tokens into an embedding. Next step is to apply the self-attention operation. GPT2Model defines 12 attention blocks starting with the layer norm.\n\nLayer Norm (ln_1)\n\nEach transformer block starts with a layer normalization steps that have parameters for scaling and shifting the normalized output, each adding (2D) parameters per block.\n\n\nNo.¬†of param = 2xD = 2x768 = 1,536\n\n\n\nAttention c_attn\n\nThe attention mechanism has ( H ) heads, each with its own set of weights for query, key, and value transformations. While each head has a dimension of D/h since there are h heads, it can be simplified to D. Since there are three matrices query, key, value and a bias vector the numbers of parameters in c_attn block could be represented by the expression [Dx(3D)+3D]\n\n\n\nc_proj\n\nc_proj combines the output of multiple heads. Its a size of DxD matrix and an additional bias vector D.\n\n\nNo.¬†of param in c_attn = [Dx(3D)+(3D)] = 1,771,776 No.¬†of param in c_proj = [DxD+D] = 590,592 params = c_attn + c_proj = [Dx(3D)+(3D)] + [DxD+D] = 2,362,368\n\n\nc_attn_params = sum(p.numel() for p in model._modules['h'][0].attn.c_attn.parameters())\nc_proj_params = sum(p.numel() for p in model._modules['h'][0].attn.c_proj.parameters())\nprint(\"number of params in c_attn: \", n.numerize(c_attn_params ))\nprint(\"number of params in c_proj: \", n.numerize(c_proj_params ))\nprint(\"sum of c_attn and c_proj\", n.numerize(c_attn_params+c_proj_params))\n\nnumber of params in c_attn:  1.77M\nnumber of params in c_proj:  590.59K\nsum of c_attn and c_proj 2.36M\n\nGoing through the definition of attn in the architecture you will come across attn_dropout and resid_dropout. I skip their description since they do not add any additional parameters.\n\n\n\nLayer Norm (ln_2)\nIts similar to Layer Norm ln_1.\n\n\nFeed Forward Neural Network mlp\n\nThe feed-forward neural network has two linear transformations with a non-linear activation in between. c_fc is responsible for projecting the output of the Attention layer into a hidden space 4xD. The c_fc is of size Dx4D plus a bias vector of size 4D.  Next, c_proj is responsible for projecting the output of the first feed-forward layer back into the embedding space. Therefore c_proj is equal to 4DxD plus a bias vector of size D.\n\n\nc_fc = [Dx(4xD)+(4xD)] = 2,362,368 c_proj = [(4xD)xD+D] = 2,360,064 No.¬†of param = [Dx(4D)+(4D)] + [(4D)xD+D] = 4,722,432\n\n\nc_fc_params = sum(p.numel() for p in model._modules['h'][0].mlp.c_fc.parameters())\nc_proj_params = sum(p.numel() for p in model._modules['h'][0].mlp.c_proj.parameters())\nprint(\"number of params in c_attn: \", n.numerize(c_fc_params ))\nprint(\"number of params in c_proj: \", n.numerize(c_proj_params ))\nprint(\"number of params in mlp\", n.numerize(c_fc_params +c_proj_params))\n\nnumber of params in c_attn:  2.36M\nnumber of params in c_proj:  2.36M\nnumber of params in mlp 4.72M\nGoing through the definition of MLP in the architecture you will come across act and dropoff. I skip their description since they do not add any additional parameters.\n\n\nLayer Norm (ln_f)\nIts similar to Layer Norm ln_1"
  },
  {
    "objectID": "posts/transformer-params/index.html#total-number-of-parameters-in-gpt2",
    "href": "posts/transformer-params/index.html#total-number-of-parameters-in-gpt2",
    "title": "Counting Number of Parameters in GPT2",
    "section": "Total number of parameters in GPT2",
    "text": "Total number of parameters in GPT2\nTo calculate params in GPT2 we have to add all the parameters we have come across.\n\\[No. of param = w_{\\text{te}} + c_{\\text{te}} + L \\times (ln_1 + \\text{attn} + ln_2 + \\text{mlp}) + ln_f\\] \\[= 38,597,376 + 786,432 + 12 \\times (1,536 + 2,362,368 + 1,536 + 4,722,432) + 1,536\\] \\[ = 124,439,808\\]\nSo the final calculation is the same as the first operation we did to determine number of parameters. Using the process we can get a ballpark estimate of number of parameter‚Äôs for model of any size."
  },
  {
    "objectID": "posts/transformer-mha/index.html",
    "href": "posts/transformer-mha/index.html",
    "title": "Building Blocks of Transformers: 3. Extending single-head attention to multi-head attention",
    "section": "",
    "text": "The attention module is typically extended into multiple attention modules sometimes referred to as attention heads. Multiple attention heads operate independently allowing the model to process different part of sequence differently.\nStacking multiple single-head attention layers\n\nThe transformer encoder is a stack of multiple attention heads \\(num\\_head\\) number of times.\n\n\n\n\nmulti-head-attention\n\n\n\nAfter computing the attention weights and context vectors form all heads are transposed to the shape of (b, num_tokens, num_heads, head_dim). The vectors are flattened into (b, num_tokens, d_out), effectively combining the outputs from all heads.\n\nBenefits of Multi-Head Attention\nDiverse Perspectives: By having multiple attention heads, the model can view the input data from different perspectives, allowing it to capture more complex patterns.\nReduced Overfitting: The ability to focus on various aspects of the data can help reduce the risk of overfitting, as the model doesn‚Äôt rely solely on a single representation.\nImproved Performance: Multi-head attention often leads to better performance in tasks like translation, summarization, and text classification, owing to its flexibility and robustness.\nReferences:\n\n1). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: ‚ÄúAttention Is All You Need‚Äù, 2017; arXiv:1706.03762"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Byte Sized ML",
    "section": "",
    "text": "Counting Number of Parameters in GPT2\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nGPT2\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nHaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Blocks of Transformers: 3. Extending single-head attention to multi-head attention\n\n\n\n\n\n\n\nMultihead-Attention\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Blocks of Transformers: 2. Position Representation\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nHaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Blocks of Transformers: 1. Self Attention\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nHaad Khan\n\n\n\n\n\n\nNo matching items"
  }
]
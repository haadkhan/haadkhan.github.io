[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Haad, an applied scientist working at the intersection of LLM and Healthcare. Outside of work he can be found on a sand court playing üèê or pickleball üèì\nThe name of the blog ‚Äúbyte sized ML‚Äù is inspired from byte sized engineering https://www.bytesizedengineering.com/ ## Education\nUniversity of Michigan | Ann Arbor MI MS in NLP & Information Retreival | 2014 - 2016\nGIK University B.Eng | Aug 2010 - May 2014"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nOracle | Sr.¬†Applied Scientist | Mar 2018 - Nov 2023 Cerebri AI | Data Scientist | Feb 2016 - Feb 2018"
  },
  {
    "objectID": "posts/transformer-components/index.html",
    "href": "posts/transformer-components/index.html",
    "title": "Building Blocks of Transformers: 2. Position Representation",
    "section": "",
    "text": "1. Positional Encoding Given the sequence He can always be seen working hard. This sequence is distinctly different from He can hardly be seen working. As can be seen slight change in the position of the word conveys a drastically different meaning.\nRNN‚Äôs are traditionally deployed for NLP because the order of the sequence defines the order of the rollout. Thus it is capable of representing two different sequences. Vanilla Self-attention lacks the notion of the order of the sequence. This can be seen from the attention operation on both the sequences. The first sequence \\(s_0\\) can be represented as set of vectors \\[\n\\textbf{x}_{1:n} = [ \\textbf{x}_{He};\\textbf{x}_{can};\\textbf{x}_{always};\\textbf{x}_{be} ;\\textbf{x}_{seen};\\textbf{x}_{working};\\textbf{x}_{hard};\\textbf{x}_{s1}] \\in \\mathrm{R^{7xd}}\n\\] The softmax or the weight \\(\\alpha_{s_{1,0}}\\) by which we look up first word by \\[\n\\alpha_{s1} = softmax([q^\\intercal_{s1}k_{He};q^\\intercal_{s1}k_{can};q^\\intercal_{s1}k_{always};\nq^\\intercal_{s1}k_{be} ;\nq^\\intercal_{s1}k_{seen};\nq^\\intercal_{s1}k_{working};\nq^\\intercal_{s1}k_{hard};\nq^\\intercal_{s1}k_{s1}])\n\\] For the reordered sentence \\(s_0\\) He can hardly be seen working. The \\(\\alpha_{s2}\\) will be identical to attention of the first sequence since the numerator hasn‚Äôt changed, and the denominator hasn‚Äôt changed; only the terms are rearranged in the sum. This all comes back down to the two facts that (1) the representation of x is not position- dependent; it‚Äôs just Ew for whatever word w, and (2) there‚Äôs no dependence on position in the self-attention operations.\nRepresentating Position via learned embedding&lt;&gt; A common way to encode the position of the word/token is to encode positional information in the input vector itself. \\[\n\\tilde{x_{i}} = P_{i} + x_{i}\n\\] And perform self attention on the position encoded vector.\n2 Nonlinear element-wise operation Consider the scenario where multiple layers of self-attention are piled on top of each other. Could this arrangement potentially serve as a substitute for the traditionally used stacked LSTM layers? There appears to be a missing component when we think about this intuitively - the elementwise nonlinear functions that are a staple in typical deep learning models. Interestingly, when you combine two layers of self-attention, the resulting structure bears a striking resemblance to that of a solitary self-attention layer.\nIt‚Äôs usual to follow a self-attention layer with a feed-forward network that processes each word representation individually. \\[\nh_{FF} = W_2 ReLU(W1h_{self-attention} + b1) + b2\n\\] 3 Future masking\nIn a Transformer, there‚Äôs nothing explicit in the self-attention weight Œ± that says not to look at indices j &gt; i when representing token i. In practice, we enforce this constraint simply adding a large negative constant to the input to the softmax.\n\\[\na_{ij,masked} = \\left\\{ \\begin{array}{cl}\na_{ij} & : \\ j \\leq i \\\\\n0 & : \\ otherwise\n\\end{array} \\right.\n\\]"
  },
  {
    "objectID": "posts/transformer/index.html",
    "href": "posts/transformer/index.html",
    "title": "Building Blocks of Transformers: 1. Self Attention",
    "section": "",
    "text": "I had a garbled understanding of Transformer architecture after consulting blogs, videos and coursera course. What made it finally clicked for me is Stanford CS 224N lecture by John Hewitt. He does a phenomenal job and I will strongly encourage to check it out. If you work with LLM‚Äôs the 1 hour and 17 minutes is worth the time investment.\n\nStanford CS224N NLP | Lecture 8 - Self-Attention and Transformers\n\nBefore diving into the details of transformer a little history.\n\n\nMotivation for Transformers\n\nPrior to introduction of Transformers, the state of the art algorithm for acheiving state of the art results on various NLP tasks were RNN‚Äôs and its variants e.g LSTM, BiDirectional LSTM etc. While the sequential nature of RNN lend itself well to modelling sequential data, it had some issues. The major ones being the following.\na). Linear interaction distance\nb). Lack of parallelizability\nTransformer solve the above problems but have their own issues which will come up later. Transformer is illustrated by the diagram below. \nThe diagram is complex but when approached as a submodules the algorithm is composed of its starts to make sense. In this note the submodule we will look at is Self-Attention.\nThe words from natural language go through a text tokenizer. Numerous technqiues exist for tokenize, but a very common method for tokenizing text is byte pair encoding. Here is a good explainer for byte pair encoding.\nThe tokenizer will transform a sentence into a list of numeral integers or a list of tokens. Given a token \\(x_i\\) in the list of tokens \\(x_1:n\\) we define a query \\(q_i = Qx_i\\), for matrix Q. A vector key and value are also needed such that \\(k_i = Kx_i\\) and \\(v_i = Vx_i\\).\nThe eventual representation of the list of tokens \\(h_i\\) is dot product of the value of the sequence. \\[\nh_i = \\sum_{j=1}^{n} \\alpha_{ij} v_j,\n\\] The weights \\(\\alpha_{ij}\\) selects the data to pull up. The weights are defined by calculating the relation between key and query \\(q_i^Tk_j\\) and calculating the softmax over the sequence. \\[\n\\alpha_ij = \\frac {exp(q_i^T k_j)}{\\sum_{j^`=1}^{n}exp(q_i^Tk_{j^`})}\n\\]\nSelf-attention instead of using a fixed embedding for each token, leverages the whole sequence to determine what other tokens should represent \\(x_i\\) in context.\nReferences:\n\n1). https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#/media/File:The-Transformer-model-architecture.png\n\n\n2). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: ‚ÄúAttention Is All You Need‚Äù, 2017; arXiv:1706.03762\n\n\n3). Stanford CS224N NLP\n\n\n4). Stanford CS224N NLP | Self-Attention and Transformers\n\n\n5). https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
  },
  {
    "objectID": "posts/transformer-mha/index.html",
    "href": "posts/transformer-mha/index.html",
    "title": "Building Blocks of Transformers: 3. Extending single-head attention to multi-head attention",
    "section": "",
    "text": "The attention module is typically extended into multiple attention modules sometimes referred to as attention heads. Multiple attention heads operate independently allowing the model to process different part of sequence differently.\nStacking multiple single-head attention layers\n\nThe transformer encoder is a stack of multiple attention heads \\(num\\_head\\) number of times.\n\n\n\n\nmulti-head-attention\n\n\n\nAfter computing the attention weights and context vectors form all heads are transposed to the shape of (b, num_tokens, num_heads, head_dim). The vectors are flattened into (b, num_tokens, d_out), effectively combining the outputs from all heads.\n\nBenefits of Multi-Head Attention\nDiverse Perspectives: By having multiple attention heads, the model can view the input data from different perspectives, allowing it to capture more complex patterns.\nReduced Overfitting: The ability to focus on various aspects of the data can help reduce the risk of overfitting, as the model doesn‚Äôt rely solely on a single representation.\nImproved Performance: Multi-head attention often leads to better performance in tasks like translation, summarization, and text classification, owing to its flexibility and robustness.\nReferences:\n\n1). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: ‚ÄúAttention Is All You Need‚Äù, 2017; arXiv:1706.03762"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Byte Sized ML",
    "section": "",
    "text": "Building Blocks of Transformers: 3. Extending single-head attention to multi-head attention\n\n\n\n\n\n\n\nMultihead-Attention\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Blocks of Transformers: 2. Position Representation\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nHaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Blocks of Transformers: 1. Self Attention\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nHaad Khan\n\n\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Haad, an applied scientist working at the intersection of LLM and Healthcare. Outside of work he can be found on a sand court playing üèê or pickleball üèì\nThe name of the blog ‚Äúbyte sized ML‚Äù is inspired from byte sized engineering https://www.bytesizedengineering.com/ ## Education\nUniversity of Michigan | Ann Arbor MI MS in NLP & Information Retreival | 2014 - 2016\nGIK University B.Eng | Aug 2010 - May 2014"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nOracle | Sr.¬†Applied Scientist | Mar 2018 - present Cerebri AI | Data Scientist | Feb 2016 - Feb 2018"
  },
  {
    "objectID": "posts/transformer-components/index.html",
    "href": "posts/transformer-components/index.html",
    "title": "Building Blocks of Transformers: 2. Position Representation",
    "section": "",
    "text": "Given the sequence ‚ÄúHe can always be seen working hard.‚Äù This sequence is distinctly different from ‚ÄúHe can hardly be seen working‚Äù. Subtle change in the position of the word can make a distinct difference in the meaning.\nRNN‚Äôs are traditionally deployed for NLP because the order of the sequence defines the order of the rollout. Thus it is capable of representing two different sequences.\nSelf attention is devoid of the notion of the order of the sequence.\nPosition representation through learned embeddings. A common way to encode the position of the word/token is to use vectors that are already position dependent P ‚àà RN√ód,\nWe then simply add embedded representation of the position of a word to its word embedding: Àúxi = Pi + xi (12)\nand perform self-attention as we otherwise would. Now, the self- attention operation can use the embedding Pi to look at the word at position i differently than if that word were at position j. This is done, e.g., in the BERT paper [ Devlin et al., 2019]\nAnother way to do this is to change the self attention operation itself.\n2.3 Elementwise nonlinearity"
  },
  {
    "objectID": "posts/transformer/index.html",
    "href": "posts/transformer/index.html",
    "title": "Building Blocks of Transformers: 1. Self Attention",
    "section": "",
    "text": "I had a garbled understanding of Transformer architecture after consulting blogs, videos and coursera course. What made it finally clicked for me is Stanford CS 224N lecture by John Hewitt. He does a phenomenal job and I will strongly encourage to check it out. If you work with LLM‚Äôs the 1 hour and 17 minutes is worth the time investment.\n\nStanford CS224N NLP | Lecture 8 - Self-Attention and Transformers\n\nBefore diving into the details of transformer a little history.\n\n\nMotivation for Transformers\n\nPrior to introduction of Transformers, the state of the art algorithm for acheiving state of the art results on various NLP tasks were RNN‚Äôs and its variants e.g LSTM, BiDirectional LSTM etc. While the sequential nature of RNN lend itself well to modelling sequential data, it had some issues. The major ones being the following.\na). Linear interaction distance\nb). Lack of parallelizability\nTransformer solve the above problems but have their own issues which will come up later. Transformer is illustrated by the diagram below. \nThe diagram is complex but when approached as a submodules the algorithm is composed of its starts to make sense. In this note the submodule we will look at is Self-Attention.\nThe words from natural language go through a text tokenizer. Numerous technqiues exist for tokenize, but a very common method for tokenizing text is byte pair encoding. Here is a good explainer for byte pair encoding.\nThe tokenizer will transform a sentence into a list of numeral integers or a list of tokens. Given a token \\(x_i\\) in the list of tokens \\(x_1:n\\) we define a query \\(q_i = Qx_i\\), for matrix Q. A vector key and value are also needed such that \\(k_i = Kx_i\\) and \\(v_i = Vx_i\\).\nThe eventual representation of the list of tokens \\(h_i\\) is dot product of the value of the sequence. \\[\nh_i = \\sum_{j=1}^{n} \\alpha_{ij} v_j,\n\\] The weights \\(\\alpha_{ij}\\) selects the data to pull up. The weights are defined by calculating the relation between key and query \\(q_i^Tk_j\\) and calculating the softmax over the sequence. \\[\n\\alpha_ij = \\frac {exp(q_i^T k_j)}{\\sum_{j^`=1}^{n}exp(q_i^Tk_{j^`})}\n\\]\nSelf-attention instead of using a fixed embedding for each token, leverages the whole sequence to determine what other tokens should represent \\(x_i\\) in context.\nReferences:\n\n1). https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#/media/File:The-Transformer-model-architecture.png\n\n\n2). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: ‚ÄúAttention Is All You Need‚Äù, 2017; arXiv:1706.03762\n\n\n3). Stanford CS224N NLP\n\n\n4). Stanford CS224N NLP | Self-Attention and Transformers\n\n\n5). https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Byte Sized ML",
    "section": "",
    "text": "Building Blocks of Transformers: 2. Position Representation\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nHaad Khan\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Blocks of Transformers: 1. Self Attention\n\n\n\n\n\n\n\nAttention\n\n\nTransformers\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nHaad Khan\n\n\n\n\n\n\nNo matching items"
  }
]